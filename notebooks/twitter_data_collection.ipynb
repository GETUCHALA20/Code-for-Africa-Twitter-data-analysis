{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tigrai genocide hashtag extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NtG_p1nUu5M",
        "outputId": "6d8239d1-0005-49e2-ae07-651b070220aa"
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/17/9d/71bd016a9edcef8860c607e531f30bd09b13103c7951ae73dd2bf174163c/tweet_preprocessor-0.6.0-py3-none-any.whl\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5drUS9tYKzIw",
        "outputId": "0553ebc1-08a2-40f6-d609-67b9fe24a525"
      },
      "source": [
        "#Import necessary libraries\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import string\n",
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "from collections import Counter\n",
        "from datetime import datetime, date, time, timedelta\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import tweepy\n",
        "from tweepy.streaming import StreamListener\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy import Stream\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import preprocessor as p\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# to view all columns\n",
        "pd.set_option(\"display.max.columns\", None)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndaoWCurMSOo"
      },
      "source": [
        "TWITTER_API_KEY = \"your api key\"\n",
        "TWITTER_API_SECRET = \"your api secret key\"\n",
        "TWITTER_ACCESS_TOKEN = \"your access token\"\n",
        "TWITTER_ACCESS_TOKEN_SECRET = \"your access token secret key\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fABFvT-OUZt"
      },
      "source": [
        "class TweetSearch():\n",
        "    '''\n",
        "    This is a basic class to search and download twitter data.\n",
        "    You can build up on it to extend the functionalities for more \n",
        "    sophisticated analysis\n",
        "    '''\n",
        "    def __init__(self, cols=None,auth=None):\n",
        "        #\n",
        "        if not cols is None:\n",
        "            self.cols = cols\n",
        "        else:\n",
        "            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', \n",
        "                    'sentiment','polarity','subjectivity', 'lang',\n",
        "                    'favorite_count', 'retweet_count', 'original_author',   \n",
        "                    'possibly_sensitive', 'hashtags',\n",
        "                    'user_mentions', 'place', 'place_coord_boundaries']\n",
        "            \n",
        "        if auth is None:\n",
        "            \n",
        "            #Variables that contains the user credentials to access Twitter API \n",
        "            consumer_key = TWITTER_API_KEY\n",
        "            consumer_secret = TWITTER_API_SECRET\n",
        "            access_token = TWITTER_ACCESS_TOKEN\n",
        "            access_token_secret =TWITTER_ACCESS_TOKEN_SECRET\n",
        "            \n",
        "\n",
        "\n",
        "            #This handles Twitter authetification and the connection to Twitter Streaming API\n",
        "            auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "            auth.set_access_token(access_token, access_token_secret)\n",
        "            \n",
        "\n",
        "        #            \n",
        "        self.auth = auth\n",
        "        self.api = tweepy.API(auth,wait_on_rate_limit=True) \n",
        "        self.filtered_tweet = ''\n",
        "            \n",
        "\n",
        "    def clean_tweets(self, twitter_text):\n",
        "\n",
        "        #use pre processor\n",
        "        tweet = p.clean(twitter_text)\n",
        "\n",
        "         #HappyEmoticons\n",
        "        emoticons_happy = set([\n",
        "            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "            '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "            '<3'\n",
        "            ])\n",
        "\n",
        "        # Sad Emoticons\n",
        "        emoticons_sad = set([\n",
        "            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "            ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "            ':c', ':{', '>:\\\\', ';('\n",
        "            ])\n",
        "\n",
        "        #Emoji patterns\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                 u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                 u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                 u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                 u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                 u\"\\U00002702-\\U000027B0\"\n",
        "                 u\"\\U000024C2-\\U0001F251\"\n",
        "                 \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        #combine sad and happy emoticons\n",
        "        emoticons = emoticons_happy.union(emoticons_sad)\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        word_tokens = nltk.word_tokenize(tweet)\n",
        "        #after tweepy preprocessing the colon symbol left remain after      \n",
        "        #removing mentions\n",
        "        tweet = re.sub(r':', '', tweet)\n",
        "        tweet = re.sub(r'‚Ä¶', '', tweet)\n",
        "\n",
        "        #replace consecutive non-ASCII characters with a space\n",
        "        tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
        "\n",
        "        #remove emojis from tweet\n",
        "        tweet = emoji_pattern.sub(r'', tweet)\n",
        "\n",
        "        #filter using NLTK library append it to a string\n",
        "        filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
        "\n",
        "        #looping through conditions\n",
        "        filtered_tweet = []    \n",
        "        for w in word_tokens:\n",
        "        #check tokens against stop words , emoticons and punctuations\n",
        "            if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
        "                filtered_tweet.append(w)\n",
        "\n",
        "        return ' '.join(filtered_tweet)            \n",
        "\n",
        "    def get_tweets(self, keyword,from_date,until, csvfile=None):\n",
        "        f = open('drive/MyDrive/TigraiGenocide.csv', 'w')\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'created_at', 'source', 'original_text','clean_text', \n",
        "                    'sentiment','polarity','subjectivity', 'lang',\n",
        "                    'favorite_count', 'retweet_count', 'original_author',   \n",
        "                    'possibly_sensitive', 'hashtags',\n",
        "                    'user_mentions', 'place', 'place_coord_boundaries'])\n",
        "        \n",
        "        df = pd.DataFrame(columns=self.cols)\n",
        "\n",
        "        #page attribute in tweepy.cursor and iteration\n",
        "        for page in tweepy.Cursor(self.api.search, q=keyword, include_rts=False,tweet_mode='extended',since=from_date,until=until ).pages():\n",
        "\n",
        "            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached\n",
        "            for status in page:\n",
        "                \n",
        "                new_entry = []\n",
        "                status = status._json\n",
        "                \n",
        "                #if this tweet is a retweet update retweet count\n",
        "                if status['created_at'] in df['created_at'].values:\n",
        "                    i = df.loc[df['created_at'] == status['created_at']].index[0]\n",
        "                    #\n",
        "                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']\n",
        "                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']\n",
        "                    if cond1 or cond2:\n",
        "                        df.at[i, 'favorite_count'] = status['favorite_count']\n",
        "                        df.at[i, 'retweet_count'] = status['retweet_count']\n",
        "                    continue\n",
        "\n",
        "                #calculate sentiment\n",
        "                filtered_tweet = self.clean_tweets(status['full_text'])\n",
        "                blob = TextBlob(filtered_tweet)\n",
        "                Sentiment = blob.sentiment     \n",
        "                polarity = Sentiment.polarity\n",
        "                subjectivity = Sentiment.subjectivity\n",
        "\n",
        "                new_entry += [status['id'], status['created_at'],\n",
        "                              status['source'], status['full_text'], filtered_tweet, \n",
        "                              Sentiment,polarity,subjectivity, status['lang'],\n",
        "                              status['favorite_count'], status['retweet_count']]\n",
        "\n",
        "                new_entry.append(status['user']['screen_name'])\n",
        "\n",
        "                try:\n",
        "                    is_sensitive = status['possibly_sensitive']\n",
        "                except KeyError:\n",
        "                    is_sensitive = None\n",
        "\n",
        "                new_entry.append(is_sensitive)\n",
        "\n",
        "                hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])\n",
        "                new_entry.append(hashtags) #append the hashtags\n",
        "\n",
        "                #\n",
        "                mentions = \", \".join([mention['screen_name'] for mention in status['entities']['user_mentions']])\n",
        "                new_entry.append(mentions) #append the user mentions\n",
        "\n",
        "                try:\n",
        "                    xyz = status['place']['bounding_box']['coordinates']\n",
        "                    coordinates = [coord for loc in xyz for coord in loc]\n",
        "                except TypeError:\n",
        "                    coordinates = None\n",
        "                #\n",
        "                new_entry.append(coordinates)\n",
        "\n",
        "                try:\n",
        "                    location = status['user']['location']\n",
        "                except TypeError:\n",
        "                    location = ''\n",
        "                #\n",
        "                new_entry.append(location)\n",
        "                writer.writerow(new_entry)\n",
        "                #now append a row to the dataframe\n",
        "                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)\n",
        "                df = df.append(single_tweet_df, ignore_index=True)\n",
        "\n",
        "        #\n",
        "        df['timestamp'] = df.created_at.map(pd.Timestamp)\n",
        "        df = df.sort_values('timestamp').set_index('timestamp')\n",
        "        df = df.drop('id',axis=1)\n",
        "        \n",
        "        if not csvfile is None:\n",
        "            #save it to file\n",
        "            df.to_csv(csvfile,mode='a', index=True, encoding=\"utf-8\")\n",
        "            \n",
        "\n",
        "        return df \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7RFxz31OmKy"
      },
      "source": [
        "file_name = 'tigray_genocide.csv'\n",
        "search_words = \"#TigrayGenocide\"\n",
        "date_since = \"2021-04-13\"\n",
        "until = \"2021-04-15\"\n",
        "\n",
        "ts = TweetSearch() \n",
        "df = ts.get_tweets(search_words,date_since,until,csvfile=file_name)\n",
        "print(\"Done\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_echxqfjogW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}